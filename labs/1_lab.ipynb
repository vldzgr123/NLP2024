{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа 1. Извлечение признаков из текстовых данных\n",
    "\n",
    "\n",
    "## Задание\n",
    "- Реализовать алгоритм получения контекстных эмбеддингов (векторных представлений) слов.\n",
    "\n",
    "- Реализовать один из алгоритмов CBoW или SkipGram для получения собственных моделей Word2Vec для выбранных данных.\n",
    "\n",
    "- Визуализировать векторные представления в малой размерности.\n",
    "\n",
    "\n",
    "\n",
    "## Ход работы\n",
    "1. Загрузить коллекцию текстовых документов: просьбы жильцов. Можете использовать не весь датасет (подмножество строк таблицы). В этой ЛР вас интересует столбец с текстом обращений жильцов. Можете загрузить любой датасет для классификации текстовых данных.\n",
    "\n",
    "2. Рассмотреть каждый текстовый документ отдельно. Выполнить предварительную обработку текстовых данных (как в ЛР 8 по ML). Токенизация, лемматизация (стемминг), удаление стоп-слов.\n",
    "\n",
    "3. Составить матрицу контекстных эмбеддингов слов.\n",
    "\n",
    "4. С помощью PCA уменьшить размер контекстных эмбеддингов (размер задать самостоятельно).\n",
    "\n",
    "5. С помощью PyTorch реализовать один из алгоритмов Word2Vec: CBoW или SkipGram. Получить векторные представления текстов с помощью собственной реализации одного из этих алгоритмов.\n",
    "\n",
    "6. Сохранить полученные различными способами векторные представления в файлы tsv. Визуализировать их с помощью сервиса Projector от авторов TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from pymorphy3 import MorphAnalyzer\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Загрузить коллекцию текстовых документов: просьбы жильцов. Можете использовать не весь датасет (подмножество строк таблицы). В этой ЛР вас интересует столбец с текстом обращений жильцов. Можете загрузить любой датасет для классификации текстовых данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/Petitions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 59889 entries, 0 to 59888\n",
      "Data columns (total 3 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   id                    59889 non-null  int64 \n",
      " 1   public_petition_text  59889 non-null  object\n",
      " 2   reason_category       59889 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>public_petition_text</th>\n",
       "      <th>reason_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3168490</td>\n",
       "      <td>снег на дороге</td>\n",
       "      <td>Благоустройство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3219678</td>\n",
       "      <td>очистить кабельный киоск от рекламы</td>\n",
       "      <td>Благоустройство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2963920</td>\n",
       "      <td>Просим убрать все деревья и кустарники, которы...</td>\n",
       "      <td>Благоустройство</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3374910</td>\n",
       "      <td>Неудовлетворительное состояние парадной - надп...</td>\n",
       "      <td>Содержание МКД</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3336285</td>\n",
       "      <td>Граффити</td>\n",
       "      <td>Благоустройство</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                               public_petition_text  reason_category\n",
       "0  3168490                                     снег на дороге  Благоустройство\n",
       "1  3219678                очистить кабельный киоск от рекламы  Благоустройство\n",
       "2  2963920  Просим убрать все деревья и кустарники, которы...  Благоустройство\n",
       "3  3374910  Неудовлетворительное состояние парадной - надп...   Содержание МКД\n",
       "4  3336285                                           Граффити  Благоустройство"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"id\", \"reason_category\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Рассмотреть каждый текстовый документ отдельно. Выполнить предварительную обработку текстовых данных (как в ЛР 8 по ML). Токенизация, лемматизация (стемминг), удаление стоп-слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаление всех символов, которые не являются цифрами, буквами и пробелами, и токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()\n",
    "stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "\n",
    "def preProccesingData(text):\n",
    "    only_letters = re.sub(\"[^\\s^\\w]+\", \"\", text)\n",
    "    tokenized = word_tokenize(only_letters)\n",
    "    normalized = [morph.normal_forms(word)[0] for word in tokenized]\n",
    "    no_stop_words = [\n",
    "        word\n",
    "        for word in normalized\n",
    "        if ((word not in string.punctuation) or (word not in stopwords))\n",
    "    ]\n",
    "    return no_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>public_petition_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[снег, на, дорога]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[очистить, кабельный, киоск, от, реклама]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[просить, убрать, всё, дерево, и, кустарник, к...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[неудовлетворительный, состояние, парадный, на...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[граффити]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                public_petition_text\n",
       "0                                 [снег, на, дорога]\n",
       "1          [очистить, кабельный, киоск, от, реклама]\n",
       "2  [просить, убрать, всё, дерево, и, кустарник, к...\n",
       "3  [неудовлетворительный, состояние, парадный, на...\n",
       "4                                         [граффити]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['public_petition_text'] = data['public_petition_text'].apply(preProccesingData)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Составить матрицу контекстных эмбеддингов слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=data['public_petition_text'], min_count=1, vector_size=64, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = []\n",
    "words = list(model.wv.index_to_key)\n",
    "\n",
    "for word in words:\n",
    "    embedding_matrix.append(model.wv[word])\n",
    "\n",
    "embedding_matrix = np.array(embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4277917 ,  1.6945407 , -0.48240417, ...,  1.3330537 ,\n",
       "        -0.86998785, -1.7137235 ],\n",
       "       [ 0.40314075,  1.6296992 , -2.2135773 , ...,  3.2638073 ,\n",
       "        -1.7394531 ,  0.7052896 ],\n",
       "       [ 0.03249288,  1.4302204 , -0.2724718 , ..., -0.44682106,\n",
       "        -0.99575657, -1.3006419 ],\n",
       "       ...,\n",
       "       [ 0.11265665,  0.12962668,  0.05227232, ..., -0.01229966,\n",
       "        -0.09140461,  0.11374051],\n",
       "       [ 0.14873748,  0.0490455 ,  0.02499888, ..., -0.03449244,\n",
       "        -0.05412512,  0.02368988],\n",
       "       [ 0.02410963, -0.08244315, -0.04862258, ..., -0.11299713,\n",
       "        -0.26015386, -0.22183876]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. С помощью PCA уменьшить размер контекстных эмбеддингов (размер задать самостоятельно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(32)\n",
    "pca_data = pca.fit_transform(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10245, 32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. С помощью PyTorch реализовать один из алгоритмов Word2Vec: CBoW или SkipGram. Получить векторные представления текстов с помощью собственной реализации одного из этих алгоритмов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
